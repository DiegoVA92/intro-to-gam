[["index.html", "Workshop: Introduction to GAMs Preamble A little about me Setup", " Workshop: Introduction to GAMs Diego Villa 2021-06-18 Preamble This document contains a very basic introduction to Generalized Additive Models (GAMs). This topic was part of a series of workshops given to the Journal Club UNALM (2021-I) community (mentees, mentors and coordinators). The audience came from a very diverse set of scientific backgrounds, so the contents here are aimed to people with a basic knowledge of applied statistical analysis, although some notions about regression analysis would be preferable. A little about me Hi! Im Diego Villa, a peruvian statistician. Im mostly interested in the application of statistics and mathematics in public health and the biological sciences. I have a bachelor degree in Statistics from the National Agrarian University La Molina (UNALM), Lima, Peru. Currently, Im a researcher associated to the Health Innovation Lab at the Institute of Tropical Medicine Alexander von Humboldt at Cayetano Heredia University in Lima, Peru. Some statistical topics I enjoy learning about are Bayesian spatio-temporal modeling and causal inference. On a more personal level, I love music. It is one of the thing I cannot live without. I listen mostly to hard rock music, but lately Im turning to more melodic music, like indie rock and dream pop. I also enjoy watching movies, specially physiological thrillers. I try to stay fit and do exercises regularly. Biking is one of the activities I do the most. Setup "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction The task of a supervised statistical learning model is to approximate the ground truth function that governs the statistical association between a response variable, denoted by \\(Y\\), and one or more predictor variables, denoted by \\(X\\). Say the ground truth model is \\(Y = f(X) + \\epsilon\\), where \\(\\epsilon\\) is a random error. Therefore, we want to find an estimate of the function \\(f(X)\\), denoted by \\(\\hat{f}(X)\\), such that we can use it to predict or infer about the response \\(Y\\). Lets see an example. Lathrop (2013) directed a long-term sampling program, from 1976 to 1994, on the population of zooplankton at the four Yahara River chain of lakes (Mendota, Monona, Waubesa, and Kegonsa) in the state of Wisconsin, USA. Pedersen et al. (2019) analyzed a processed version of this data using Hierarchical Generalized Additive Models (HGAMs) (more on this later), and the code in R used for the analysis can be found in the Supplemental Information section of the paper. We are going to reproduce this analysis to show the main concepts about GAMs. The data set consist on measurements of the population density of 8 zooplankton taxa in the aforementioned lakes taken roughly in a biweekly schedule, although, depending on the season or weather conditions, the sampling frequency varied. The day of the week of the measurement was recorded, as well as the year. Our aim is to build a model to infer population density of a taxon at a certain day of the year in the Mendota lake. Accordingly, the population density is our response variable, and the taxon and the day of the year are our predictor variables. The sample distribution of the population density has a very right skewed distribution, with most of its density close to zero. For instance, a population density can not be less than zero. For this reason, we are a going to model the population density using a Gamma distribution, and use a Generalized Linear Model (GLM) to build a regression linear model on the log-mean of the population density. We are going to shed light on this later, our porpuse here is only to show some graphical examples. First, lets keep things simple and analyze the association between the population density and the day of the year for a particular taxon in lake Mendota. Lets select the D. thomasi taxon and build a scatterplot with the days of year. For practical reasons, we are going to plot the population density in a base 10 logarithmic scale. filepath &lt;- path(&quot;data&quot;, &quot;zooplankton.csv&quot;) zooplankton &lt;- read_csv(filepath) zoo_example &lt;- filter(zooplankton, taxon == &quot;D. thomasi&quot;, lake == &quot;Mendota&quot;) plt_thomasi &lt;- zoo_example %&gt;% ggplot(aes(x = day, y = density_adj)) + geom_point(size = 1) + scale_y_log10(breaks = c(0.1, 1, 10, 100, 1000), labels = c(&quot;0.1&quot;, &quot;1&quot;, &quot;10&quot;, &quot;100&quot;, &quot;1000&quot;)) + labs(x = &quot;Day of year&quot;, y = &quot;Population density&quot;) print(plt_thomasi) Figure 1.1: Scatterplot of the population density and the day of the day for the D. thomasi taxon in Mendota lake. In Figure 1.1 we can distinguish a pattern in the population density throughout the days, but it is clearly kind of difficult to tell its form. One thing is for sure, it isnt a linear association. Consequently, a (generalized) linear regression model would be inadequate. To check this, lets plot the linear regression fitted line. x &lt;- zoo_example$day y &lt;- zoo_example$density_adj glm_linear &lt;- glm(y ~ x, family = Gamma(link = &quot;log&quot;)) pred_linear &lt;- predict(glm_linear, type = &quot;response&quot;) dat_linear &lt;- data.frame(x = x, linear = pred_linear) plt_glm_linear &lt;- plt_thomasi + geom_line(data = dat_linear, aes(x = x, y = linear, colour = &quot;linear&quot;)) + scale_color_npg() + theme(legend.title = element_blank()) plt_glm_linear Figure 1.2: Scatterplot of the population density and the day of the day for the D. thomasi taxon in Mendota lake showing the linear regression fitted line. From 1.2 we can check that the fitted GLM line definitely does not capture the association of the population density and the days of the year. As we stated in the initial paragraph, we need to find a function \\(\\hat{f}\\) to be able to predict the response variable. But, how do we specify this function so it accurately capture the association between the variables? For instance, in the GLM, it is assumed a linear association between the log-mean of the response and the predictor. This is a very restrictive form, but certainly useful in a variety of applications. However, we now for sure it wont be useful for our analysis. We could try polynomial regression. Lets plot a second degree and a third degree polynomial regression fitted curve, together with the linear regression fitted line . glm_poly2 &lt;- glm(y ~ poly(x, 2), family = Gamma(link = &quot;log&quot;)) pred_poly2 &lt;- predict(glm_poly2, type = &quot;response&quot;) glm_poly3 &lt;- glm(y ~ poly(x, 3), family = Gamma(link = &quot;log&quot;)) pred_poly3 &lt;- predict(glm_poly3, type = &quot;response&quot;) dat_poly &lt;- data.frame(x, pred_linear, pred_poly2, pred_poly3) dat_poly_long &lt;- pivot_longer( dat_poly, cols = !x, names_to = &quot;model&quot; ) dat_poly_long$model &lt;- factor(dat_poly_long$model, levels = c(&quot;pred_linear&quot;, &quot;pred_poly2&quot;, &quot;pred_poly3&quot;), labels = c(&quot;linear&quot;, &quot;poly(2)&quot;, &quot;poly(3)&quot;)) plt_glm_poly &lt;- plt_thomasi + geom_line(data = dat_poly_long, aes(x = x, y = value, col = model)) + scale_color_npg() + theme(legend.title = element_blank()) plt_glm_poly Figure 1.3: Scatterplot of the population density and the day of the day for the D. thomasi taxon in Mendota lake showing the linear regression fitted line and the second and third degree polynomial regression fitted curves. We see from Figure 1.3 that the third degree polynomial offers a good fit to the data. The disadvantage is that it is still a linear regression model. The only thing that changes is that the response is regressed over some transformation of the predictors, in this case, the n-th degree polynomial. This is certainly a good trick. But, what if we want to model more complex non-linear associations? We need a more flexible approach. Generalized Additive Models (GAMs) allow us to build very flexible models by using smooth functions over the predictors that can take a variety of forms and structures. Lets see them in action. We are going to add a GAM fitted smooth curve to the plot. gam_smooth &lt;- gam(y ~ s(x), family = Gamma(link = &quot;log&quot;)) pred_smooth &lt;- predict(gam_smooth, type = &quot;response&quot;, se = FALSE) dat_smooth &lt;- data.frame(dat_poly, pred_smooth) dat_smooth_long &lt;- pivot_longer( dat_smooth, cols = !x, names_to = &quot;model&quot; ) dat_smooth_long$model &lt;- factor(dat_smooth_long$model, levels = c(&quot;pred_linear&quot;, &quot;pred_poly2&quot;, &quot;pred_poly3&quot;, &quot;pred_smooth&quot;), labels = c(&quot;linear&quot;, &quot;poly(2)&quot;, &quot;poly(3)&quot;, &quot;smooth&quot;)) plt_gam_smooth &lt;- plt_thomasi + geom_line(data = dat_smooth_long, aes(x = x, y = value, col = model)) + scale_color_npg() + theme(legend.title = element_blank()) plt_gam_smooth Figure 1.4: Scatterplot of the population density and the day of the day for the D. thomasi taxon in Mendota lake showing a GAM fitted curve, second and third degree polynomial regression fitted curves and the LOESS fitted curve. One highlight from Figure 1.4 is that the GAM fitted curve is more flexible than the other curves. Unlike the polynomial regression curves, we didnt have to specify a certain degree or parameter to generate the curve. This is because the smooth curve is built non-parametrically. Another important detail is that the GAM fitted curve does not overfit the data points for the first 100 days, nor for the last 100 days, which is when the pattern is less obvious. This is an indicator that the GAM offers better generalization. Before diving to the GAMs specifics, I guess you may be wondering why we care much about finding a good curve or function to specify the associations in the data. Do we really need to bother about this? We could just run a black-box algorithm, like a random forest, and get (mostly certainly) a good prediction accuracy on the population density. For this matter, I believe that a better question would be: when do have to worry about knowing the form of the function \\(\\hat{f}\\)? Lets discuss that in the next section. References "],["inter-flex.html", "Chapter 2 Interpretability vs. flexibility", " Chapter 2 Interpretability vs. flexibility We build statistical learning models for two main reasons: inference and prediction. Inference is a broad term, and some may argue than prediction is a type of inference. But here when we talk about inference think about parameters. We want to estimate some parameters that will allow us to describe the mechanism of the model. That is, when we do inference, our aim is to build a model that help us understand the associations that may exist between the observed variables. For this matter, we would want to know the form of \\(\\hat{f}\\), as its underlying structure would rule what we learn about the associations. Another important consideration, is that we need the model to learn intuitive associations, that is, ones that wont be difficult to understand for us. That makes a model output interpretable. On the other hand, prediction is all about guessing new observations. When we build a model for prediction, all we want its that it helps us to predict new observations with good precision. For instance, there is no need to understand the parameters nor to know the form of \\(\\hat{f}\\). The holy grail of statistical modelling is to build a model that can do well in booth tasks, but in practice there exists a trade-off between the intepretability of the model and its prediction accuracy. The following scheme was taken from James et al. (2013). It depicts the trade-off between interpretability and flexibility of a number of statistical learning models. Flexibility is highly associated with prediction accuracy, as a very flexible model can learn more complex patterns and better map new observations from the inputs. Conversely, a less flexible model can only learn simple patterns, which are translated into intuitive interpretations. Figure 2.1: Trade-off between model interpretability and model flexibility for a number of models. Generalized Additive Models offer a balance between them. Figure from James et al. (2013) (fig. 2.7, p. 25) As seen in 2.1, GAMs can offer us a balance between interpretability and flexibility. In the previous section, we saw how the GAM was flexibly enough to learn the pattern in the data, and its output was similar to the polynomial linear regression models, which are simpler in form. However, the example in the previous section was just the tip of an enormous iceberg. Technically, GAMs can learn any association form. GAM terms can be specified in a large range of ways. We can include parametric and non-parametric terms, multivariable functions of predictors, random effects with several structures, as well as interaction terms. Moreover, the response variable can be modeled using a variety of theoretical distributions, that why the title of generalized. The only limitation comes also from the title. These models assume that the predictor terms have an additive effect on the response variable. This limitation, however, is negligible in most applications. In the next section we are going to define a Generalized Additive Model, and dive into its machinery. References "],["gams.html", "Chapter 3 GAMs 3.1 Definition 3.2 Smoothing 3.3 Penalization", " Chapter 3 GAMs 3.1 Definition GAMs are a extension to the Generalized Linear Models (GLMs). A GLM consists on equating some function, called the link function, of the mean of the response variable with a linear combination of the predictors, named the linear predictor. Here, the response is assumed to follow a Exponential Family (EF) distribution. GAMs extents this by letting the linear predictor to include smooth functions over the predictors as terms in the linear combination. These smooth functions allow us to specify the association between the response and the predictors using non-linear continuous forms. In general, GAMs have the following structure: \\[ \\eta\\left(\\mu\\right) = X\\beta + \\gamma_{1}(X_{1}) + \\gamma_{1}(X_{2}) + \\ldots + \\gamma_{p}(X_{p}) + \\ldots \\tag{3.1} \\] where \\(\\eta\\) is the link function, \\(\\mu = E(Y)\\), \\(X\\beta\\) represents the parameterized part of the linear predictor, as in a GLM, and the functions \\(\\eta\\) are smooth functions over different sets of predictors. Here the \\(X\\)s represent the matrix form of a set of predictors rather than single predictors. Recall that it is assumed that \\(Y\\) follows a particular EF distribution. 3.2 Smoothing The main attraction of GAMs are the smooth functions. We are left with two problems: how to specify these smooth functions and how much smoothy or wiggly should they be? To construct the smooth functions, we use their basis expansion. That means that we express each smooth function as a finite linear combination of another functions, called basis functions, which are assumed to approximate the space of functions that the smooth function is part of. Functions constructed like this are called splines. For a particular smooth function, we can write \\[ \\begin{equation} \\gamma(x) = \\sum_{j=1}^{K}{\\delta_{j}b_{j}(x)} \\tag{3.2} \\end{equation} \\] where \\(\\delta_{i}\\) are the coefficients for the linear combination of the basis functions \\(b_{i}\\). Here, \\(K\\) is the number of basis functions in the linear combination, and it represents the basis complexity or basis size. This parameter is important because it controls the form of the resulting smooth function. Greater values result in a very wiggly smooth function, whereas lower values result in a plainer smooth function. In the univariate case, if we set the basis functions to be the polynomials of \\(x\\) up to the third degree, then \\(b_{1}(x) = 1\\), \\(b_{2}(x) = x\\), \\(b_{3}(x) = x^2\\) and \\(b_{4}(x) = x^3\\), and the smooth function is represented as a third degree polynomial in \\(x\\), \\(\\gamma(x) = \\delta_{1}+\\delta_{2}x+\\delta_{3}x^2+\\delta_{4}x^3\\). Different splines can be specified using different types of basis functions. A very popular type are the B-splines which are constructed using piecewise polynomial basis functions. Lets see how the B-splines are constructed and fitted to the data in 1 for different numbers of basis size using a GLM. zoo_example &lt;- filter(zooplankton, taxon == &quot;D. thomasi&quot;, lake == &quot;Mendota&quot;) x &lt;- zoo_example$day y &lt;- zoo_example$density_adj get_bspline &lt;- function(k) { b &lt;- bs(x, df = k, degree = 2) m &lt;- glm(y ~ b, family = Gamma(link = &quot;log&quot;)) z &lt;- sweep(cbind(1, b), 2, coef(m), &quot;*&quot;) db &lt;- z[, 2:(k+1)] l &lt;- rowSums(z) g &lt;- rowSums(db) s &lt;- data.frame(x, db) return(list(b = b, l = l, db = db, g = g, s = s)) } plt_bspline &lt;- function(k) { bspline &lt;- get_bspline(k) g &lt;- bspline$g s &lt;- bspline$s s_pivot &lt;- pivot_longer(s, cols = !x, names_to = &quot;b&quot;) plt &lt;- ggplot() + geom_line(aes(x = x, y = g), lwd = 0.65) + geom_line(data = s_pivot, aes(x = x, y = value, col = b)) + geom_hline(yintercept = 0, lty = &quot;dashed&quot;) + labs(y = expression(gamma), title = paste0(&quot;K = &quot;, k)) + theme(legend.position = &quot;none&quot;) plt } plt_bs_k2 &lt;- plt_bspline(2) plt_bs_k5 &lt;- plt_bspline(5) plt_bs_k10 &lt;- plt_bspline(10) plt_bs_k20 &lt;- plt_bspline(20) grid.arrange(plt_bs_k2, plt_bs_k5, plt_bs_k10, plt_bs_k20, ncol = 2) Figure 3.1: Smooth function contructed as a B-spline using different values of basis complexity (K) In each panel of 3.1, the colored curves are the basis functions time their fitted coefficients, and the summation of these terms yield the smooth function following (3.2). We can see that increasing the basis complexity yields a less smoothed function with more wiggliness, that is, a more complex function. Lets see how these smooth functions fit the data. plt_manual_bs &lt;- function(k) { bspline &lt;- get_bspline(k) l &lt;- bspline$l dat &lt;- data.frame(x, l) plt &lt;- ggplot() + geom_point(aes(x = x, y = y), size = 1) + geom_line(data = dat, aes(x = x, y = exp(l), colour = &quot;bs&quot;)) + scale_color_npg() + scale_y_log10(breaks = c(0.1, 1, 10, 100, 1000), labels = c(&quot;0.1&quot;, &quot;1&quot;, &quot;10&quot;, &quot;100&quot;, &quot;1000&quot;)) + labs(x = &quot;Day of year&quot;, y = &quot;Population density&quot;, title = paste0(&quot;K = &quot;, k)) + theme(legend.position = &quot;none&quot;) } plt_manual_bs_k2 &lt;- plt_manual_bs(2) plt_manual_bs_k5 &lt;- plt_manual_bs(5) plt_manual_bs_k10 &lt;- plt_manual_bs(10) plt_manual_bs_k20 &lt;- plt_manual_bs(20) grid.arrange(plt_manual_bs_k2, plt_manual_bs_k5, plt_manual_bs_k10, plt_manual_bs_k20, ncol = 2) Figure 3.2: Regression B-splines in a GLM using different values of basis complexity (K) In 3.2 the see that increasing the basis size of the B-splines results in overfitting, which is something we want to avoid. On the other hand, a small basis size is too simple to capture the association. 3.3 Penalization Another way to control the complexity or wiggliness of the smooth functions in to penalize for their complexity in the objective function of the model. This is where GAMs depart from the GLMs. Recall the in a GLM, the objective function is the likelihood of the model. That is, we want to maximize it in order to get a good fit. In GAMs, the objective function is the penalized likelihood, which consists on the likelihood plus a penalty term on the complexity of the smooth functions. The penalty is on the coefficients of the basis functions in 3.1. Thus, we can write \\[ \\begin{equation} l_{p}(\\delta) = l(\\delta) + \\lambda\\delta&#39;S\\delta \\tag{3.3} \\end{equation} \\] where \\(l_{p}(\\delta)\\) is the penalized likelihood, \\(l(\\delta)\\) is the regular likelihood, and the penalized term consists on a penalization parameter or smoothing parameter \\(\\lambda\\) and a penalty matrix \\(S\\). The smoothing parameter is the one that control how much penalization we do over the complexity of the smooth functions. The penalty matrix is specific to the type of basis functions we are using, and they specify how the penalization is made. From both of them, the smoothing parameter is the most important one, and here we are going to analyze how its magnitude affect the form of the smooth function. Before diving in the analysis of the smoothing parameter, we are going to talk about briefly about the implementation of GAMs we are using in {R}. The mgcv package is the recommended option to work with, as it offers different fitting procedures, a large range of smooth functions specifications, and, perhaps the main feature, automatic estimation of the smoothing parameters. The latter is done by three methods: Generalized Cross Validation (GCV), Restricted Marginal Likelihood (REML), and Integrated Nested Laplace Approximation (INLA) for fully Bayesian inference. For general purposes, REML is the recommended option. The default option to construct the smooth functions in mgcv is using Thin Plate (TP) regression splines, because they dont depend on knot placement and we can build smooth functions of several variables with them. For an exhaustive review of GAMs and this implementation in {R} please refer to Wood (2017). A remark: the basis complexity is not penalized. Instead of that, it is usually set to a large value and then its complexity is regularized using the penalty term. Now we are going to fit GAMs to the zooplankton example data setting the basis complexity to 50 and using different values for the smooth parameter (SP). plt_gam_sp &lt;- function(sp) { m &lt;- gam(y ~ s(x, bs = &quot;tp&quot;, k = 50, sp = sp), method = &quot;REML&quot;, family = Gamma(link = &quot;log&quot;)) pred &lt;- predict(m, type = &quot;response&quot;, se = FALSE) dat &lt;- data.frame(x, pred) plt &lt;- ggplot() + geom_point(aes(x = x, y = y), size = 1) + geom_line(data = dat, aes(x = x, y = pred, colour = &quot;sp&quot;)) + scale_color_npg() + scale_y_log10(breaks = c(0.1, 1, 10, 100, 1000), labels = c(&quot;0.1&quot;, &quot;1&quot;, &quot;10&quot;, &quot;100&quot;, &quot;1000&quot;)) + labs(x = &quot;Day of year&quot;, y = &quot;Population density&quot;, title = paste0(&quot;SP = &quot;, as.character(sp))) + theme(legend.position = &quot;none&quot;) } plt_gam_sp1 &lt;- plt_gam_sp(0.0001) plt_gam_sp2 &lt;- plt_gam_sp(0.001) plt_gam_sp3 &lt;- plt_gam_sp(0.01) plt_gam_sp4 &lt;- plt_gam_sp(0.1) grid.arrange(plt_gam_sp1, plt_gam_sp2, plt_gam_sp3, plt_gam_sp4, ncol = 2) Figure 3.3: GAM models fitted using different values for the smoothing parameter. In 3.3 we observe that, despite the basis size being quite large, the smoothing parameter is essential to control the complexity of the model. A very small smoothing parameter offers less penalization, that why the model in the first plot overfits drastically the data. On the other hand, the same model with a larger smoothing parameter as in the last plot is better regularized and yields a better fit. In this section we have very briefly talked about smoothing and penalization, which are the main concepts in GAMs. We strongly encourage to the curious reader to check the work of Wood (2017), which is the most cited reference in GAMs theory and application. In the next section, we are going to work on a full example on the zooplankton data to show more of the features the GAMs have. References "],["application.html", "Chapter 4 Application", " Chapter 4 Application We are going to replicate the first example on the zooplankton data analyzed in Pedersen et al. (2019). zooplankton$taxon &lt;- factor(zooplankton$taxon) zooplankton$year_f &lt;- factor(zooplankton$year) zoo_train &lt;- subset(zooplankton, year%%2 == 0 &amp; lake == &quot;Mendota&quot;) zoo_test &lt;- subset(zooplankton, year%%2 == 1 &amp; lake == &quot;Mendota&quot;) get_deviance &lt;- function(model, y_pred, y_obs, weights = NULL){ stopifnot(length(y_obs) == length(y_pred)) if(is.null(weights)) weights = rep(1, times = length(y_obs)) dev_residuals = model$family$dev.resids(y_obs, y_pred, weights) return(sum(dev_residuals)) } zoo_comm_modS &lt;- gam( density_adj ~ s(taxon, year_f, bs = &quot;re&quot;) + s(day, taxon, bs = &quot;fs&quot;, k = 10, xt = list(bs = &quot;cc&quot;)), data = zoo_train, knots = list(day = c(0, 365)), family = Gamma(link = &quot;log&quot;), method = &quot;REML&quot;, drop.unused.levels = FALSE ) zoo_comm_modI &lt;- gam( density_adj ~ s(day, by = taxon, k = 10, bs = &quot;cc&quot;) + s(taxon, bs = &quot;re&quot;) + s(taxon, year_f, bs = &quot;re&quot;), data = zoo_train, knots = list(day = c(0, 365)), family = Gamma(link = &quot;log&quot;), method = &quot;REML&quot;, drop.unused.levels = FALSE ) plt1 &lt;- qq_plot(zoo_comm_modI, method = &quot;simulate&quot;) + labs(title = &quot;QQ-plot&quot;, subtitle = NULL) df &lt;- data.frame(log_fitted = log(fitted(zoo_comm_modI)), residuals = resid(zoo_comm_modI, type = &quot;deviance&quot;)) plt2 &lt;- ggplot(df, aes(x = log_fitted, y = residuals)) + geom_point() + labs(x = &quot;Linear predictor&quot;, y = &quot;Deviance residual&quot;, title = &quot;Residuals vs. linear predictor&quot;) plot_grid(plt1, plt2, ncol = 2) zoo_plot_data &lt;- expand.grid( day = 1:365, taxon = factor(levels(zoo_train$taxon)), year_f = 1980 ) zoo_modS_fit &lt;- predict( zoo_comm_modS, zoo_plot_data, se.fit = TRUE, exclude = &quot;s(taxon,year_f)&quot; ) zoo_modI_fit &lt;- predict( zoo_comm_modI, zoo_plot_data, se.fit = TRUE, exclude = &quot;s(taxon,year_f)&quot; ) zoo_plot_data$modS_fit &lt;- as.numeric(zoo_modS_fit$fit) zoo_plot_data$modI_fit &lt;- as.numeric(zoo_modI_fit$fit) zoo_plot_data &lt;- gather(zoo_plot_data, model, fit, modS_fit, modI_fit) zoo_plot_data &lt;- mutate( zoo_plot_data, se= c(as.numeric(zoo_modS_fit$se.fit), as.numeric(zoo_modI_fit$se.fit)), upper = exp(fit + (2 * se)), lower = exp(fit - (2 * se)), fit = exp(fit) ) zoo_plot_model_labels = paste(&quot;Model&quot;, c(&quot;S&quot;, &quot;I&quot;)) zoo_plot_model_labels = factor( zoo_plot_model_labels, levels = zoo_plot_model_labels ) zoo_plot &lt;- ggplot(zoo_plot_data) + facet_wrap(~ taxon, nrow = 4, scales = &quot;free_y&quot;)+ geom_ribbon(aes(x=day, ymin = lower, ymax = upper, fill = model), alpha=0.2) + geom_point(data = zoo_train, aes(x = day, y = density_adj), size = 0.06) + geom_point(data = zoo_test, aes(x = day, y = density_adj), size = 0.06, col = &quot;grey&quot;) + geom_line(aes(x = day, y = fit, color = model))+ labs(y = &quot;Population density&quot;, x = &quot;Day of year&quot;) + scale_y_log10(breaks = c(0.1, 1, 10, 100, 1000), labels = c(&quot;0.1&quot;, &quot;1&quot;, &quot;10&quot;, &quot;100&quot;, &quot;1000&quot;))+ scale_fill_brewer(name = &quot;&quot;, palette = &quot;Dark2&quot;, labels = zoo_plot_model_labels) + scale_colour_brewer(name = &quot;&quot;, palette = &quot;Dark2&quot;, labels = zoo_plot_model_labels)+ theme(legend.position = &quot;top&quot;) zoo_plot zoo_comm_mod0 &lt;- gam( density_adj ~ s(taxon,bs=&quot;re&quot;), data = zoo_train, knots = list(day =c(0, 365)), family = Gamma(link =&quot;log&quot;), method = &quot;REML&quot;, drop.unused.levels = FALSE ) zoo_test_summary &lt;- zoo_test %&gt;% mutate( mod0 = predict(zoo_comm_mod0, ., type=&quot;response&quot;), modS = predict(zoo_comm_modS, ., type=&quot;response&quot;), modI = predict(zoo_comm_modI, ., type=&quot;response&quot;))%&gt;% group_by(taxon)%&gt;% summarise( `Intercept only` = format(get_deviance(zoo_comm_mod0, mod0, density_adj), scientific = FALSE, digits=3), `Model S` = format(get_deviance(zoo_comm_modS, modS, density_adj), scientific = FALSE, digits=3), `Model I` = format(get_deviance(zoo_comm_modI, modI, density_adj), scientific = FALSE, digits=3)) knitr::kable(zoo_test_summary) taxon Intercept only Model S Model I C. sphaericus 715 482 495 Calanoid copepods 346 222 223 Cyclopoid copepods 569 382 386 D. mendotae 353 264 268 D. thomasi 486 333 337 K. cochlearis 486 2264 2340 L. siciloides 132 116 126 M. edax 270 138 139 References "],["resources.html", "Chapter 5 Resources Books Papers Online courses", " Chapter 5 Resources Books Wood, S. N. (2017). Generalized additive models: an introduction with R. CRC press. Hastie, T. J. (2017). Generalized additive models (pp. 249-307). Routledge. De Boor, C., &amp; De Boor, C. (1978). A practical guide to splines (Vol. 27, p. 325). New York: springer-verlag. Papers Wood, S. N. (2011). Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(1), 3-36. Wood, S. N. (2004). Stable and efficient multiple smoothing parameter estimation for generalized additive models. Journal of the American Statistical Association, 99(467), 673-686. Wood, S. N. (2003). Thin plate regression splines. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(1), 95-114. Rigby, R. A., &amp; Stasinopoulos, D. M. (2005). Generalized additive models for location, scale and shape. Journal of the Royal Statistical Society: Series C (Applied Statistics), 54(3), 507-554. Pedersen, E. J., Miller, D. L., Simpson, G. L., &amp; Ross, N. (2019). Hierarchical generalized additive models in ecology: an introduction with mgcv. PeerJ, 7, e6876. Hastie, T., &amp; Tibshirani, R. (1987). Generalized additive models: some applications. Journal of the American Statistical Association, 82(398), 371-386. Hastie, T., &amp; Tibshirani, R. (1995). Generalized additive models for medical research. Statistical methods in medical research, 4(3), 187-196. Beck, N., &amp; Jackman, S. (1998). Beyond linearity by default: Generalized additive models. American Journal of Political Science, 596-627. Ramsay, T. O., Burnett, R. T., &amp; Krewski, D. (2003). The effect of concurvity in generalized additive models linking mortality to ambient particulate matter. Epidemiology, 14(1), 18-23. Yee, T. W., &amp; Mitchell, N. D. (1991). Generalized additive models in plant ecology. Journal of vegetation science, 2(5), 587-602. Online courses Generalized Additive Models GAMs in R "],["references.html", "References", " References "]]
