# Interpretability vs. flexibility {#inter-flex}

We build statistical learning models for two main reasons: _inference_ and _prediction_. Inference is a broad term, and some may argue than prediction is a type of inference. But here when we talk about inference think about parameters. We want to estimate some parameters that will allow us to describe the mechanism of the model. That is, when we do inference, our aim is to build a model that help us understand the associations that may exist between the observed variables. For this matter, we would want to know the form of $\hat{f}$, as its underlying structure would rule what we learn about the associations. Another important consideration, is that we need the model to learn intuitive associations, that is, ones that won't be difficult to understand for us. That makes a model output _interpretable_. On the other hand, prediction is all about guessing new observations. When we build a model for prediction, all we want its that it helps us to predict new observations with good precision. For instance, there is no need to understand the parameters nor to know the form of $\hat{f}$. The holy grail of statistical modelling is to build a model that can do well in booth tasks, but in practice there exists a trade-off between the intepretability of the model and its prediction accuracy. 

The following scheme was taken from @james2013introduction. It depicts the trade-off between interpretability and _flexibility_ of a number of statistical learning models. Flexibility is highly associated with prediction accuracy, as a very flexible model can learn more complex patterns and better map new observations from the inputs. Conversely, a less flexible model can only learn simple patterns, which are translated into intuitive interpretations.

```{r inter-flex, echo=FALSE, fig.align='center', fig.cap='Trade-off between model interpretability and model flexibility for a number of models. Generalized Additive Models offer a balance between them. Figure from @james2013introduction (fig. 2.7, p. 25)'}
knitr::include_graphics(rep("images/01-inter-flex.png"))
```

As seen in \@ref(fig:inter-flex), GAMs can offer us a balance between interpretability and flexibility. In the previous section, we saw how the GAM was flexibly enough to learn the pattern in the data, and its output was similar to the polynomial linear regression models, which are simpler in form. However, the example in the previous section was just the tip of an enormous iceberg. Technically, GAMs can learn any association form. GAM terms can be specified in a large range of ways. We can include parametric and non-parametric terms, multivariable functions of predictors, random effects with several structures, as well as interaction terms. Moreover, the response variable can be modeled using a variety of theoretical distributions, that why the title of _generalized_. The only limitation comes also from the title. These models assume that the predictor terms have an _additive_ effect on the response variable. This limitation, however, is negligible in most applications.

In the next section we are going to define a Generalized Additive Model, and dive into its machinery.